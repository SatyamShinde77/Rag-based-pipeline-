ğŸš€ RAG-Based Question Answering System

A production-style Retrieval-Augmented Generation (RAG) system that allows users to upload documents (PDF/TXT) and ask intelligent questions grounded in document content.

This project demonstrates applied AI engineering, combining vector search, LLM reasoning, API design, background processing, rate limiting, performance metrics, and a client-facing UI.

ğŸ¯ Built to showcase real-world AI system design â€” not just a demo model.

ğŸ§  Key Highlights

âœ… Full RAG pipeline (Upload â†’ Chunk â†’ Embed â†’ Retrieve â†’ Generate)
âœ… FastAPI backend with clean API architecture
âœ… FAISS vector store for scalable similarity search
âœ… LLM-powered answer generation
âœ… Background ingestion jobs for performance
âœ… Rate limiting to prevent API abuse
âœ… Metrics tracking (retrieval time, generation latency, similarity)
âœ… Confidence scoring & explainability
âœ… Streamlit frontend (real product UI â€” not Swagger-only)
âœ… Resume & document analysis demo use case
âœ… Designed for internship-level evaluation & real-world relevance

ğŸ“Œ Problem Solved

Most QA systems hallucinate or answer without grounding.

This system ensures:

â¢ Answers are based only on retrieved document content

â¢ Retrieval quality is measured & optimized

â¢ Latency and performance are monitored

â¢ Users can see confidence & source relevance

ğŸ—ï¸ System Architecture

User Uploads Document
        â†“
Text Chunking (Context-Preserving)
        â†“
Embedding Generation
        â†“
Vector Storage (FAISS)
        â†“
Query Embedding + Similarity Search
        â†“
Relevant Chunk Retrieval
        â†“
LLM Answer Generation
        â†“
Answer + Confidence + Metrics + Citations

ğŸ“‚ Project Structure

ğŸ“ rag_project/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI API Server
â”‚   â”œâ”€â”€ ingestion.py     # Document chunking & embedding
â”‚   â”œâ”€â”€ retriever.py     # Similarity search (FAISS)
â”‚   â”œâ”€â”€ generator.py     # LLM answer generation
â”‚   â””â”€â”€ vector_store.py  # Vector database logic
â”‚
â”œâ”€â”€ frontend.py          # Streamlit user interface
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ README.md            # Project documentation
â””â”€â”€ EXPLANATION.md       # Technical deep-dive


âš™ï¸ Tech Stack
Backend: FastAPI, Python
Frontend: Streamlit
Embeddings: Sentence Transformers (MiniLM)
Vector Database: FAISS
LLM Providers: Groq / OpenAI / LLaMA
Retrieval: Semantic Search + Ranking
Rate Limiting: SlowAPI
Deployment Ready: Docker / Cloud Ready

ğŸš€ Features

ğŸ“¤ Document Ingestion

â¢ Accepts PDF & TXT

â¢ Chunking with overlap for better retrieval

â¢ Runs in background tasks to avoid API blocking

ğŸ” Retrieval & Search

â¢ Vector-based similarity search using FAISS

â¢ Retrieves top relevant chunks

â¢ Handles retrieval edge cases

ğŸ¤– Answer Generation

â¢ LLM answers using ONLY retrieved content

â¢ Reduces hallucination risk

â¢ Produces grounded, explainable responses

ğŸ“Š Metrics & Observability

Tracks:

â¢ Retrieval latency (ms)

â¢ Generation latency (ms)

â¢ Similarity scores

â¢ Confidence score

ğŸš« Rate Limiting

Prevents abuse:

Endpoint                                 	Limit
/upload	                               3 requests/min
/ask	                               5 requests/min

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Install Dependencies

â¢ pip install -r requirements.txt

2ï¸âƒ£ Start Backend (FastAPI)

â¢ python -m uvicorn app.main:app --reload


Open API Docs:

http://127.0.0.1:8000/docs

3ï¸âƒ£ Start Frontend (Streamlit)
python -m streamlit run frontend.py


Open UI:

http://localhost:8501


ğŸ§ª Example Queries

Try asking:

â¢ Summarize this document

â¢ What skills does this resume show?

â¢ Evaluate this candidate for an AI role

â¢ List key projects mentioned

â¢ Generate interview questions based on this resume

ğŸ“‰ Known Retrieval Failure Case (Honest Engineering Insight)

A failure occurs when:

â¢ Questions are too vague

â¢ Multiple chunks contain similar keywords

Example:

A question about projects retrieved a chunk about education

This highlights the importance of:

â¢ Chunk boundary tuning

â¢ Query specificity

â¢ Embedding precision

We improved retrieval by adding chunk overlap & ranking filters.

ğŸ“ Chunk Size Decision

Chunk size: ~500â€“700 characters

Reason:

â¢ Too small â†’ loses semantic meaning

â¢ Too large â†’ reduces retrieval precision

This balance provides:
âœ” Context completeness
âœ” Better similarity accuracy
âœ” Efficient vector search

ğŸ“Š Metric Tracked

We tracked Retrieval Latency (ms)

Why:

â¢ Measures vector search performance

â¢ Ensures fast real-time responses

â¢ Important for scalable AI systems

Average observed latency:

30â€“70 ms (FAISS-based retrieval)

ğŸ’¡ Real-World Use Cases

â¢ Resume & candidate analysis

â¢ Research paper Q&A

â¢ Legal & policy document assistant

â¢ Study material tutor

â¢ Enterprise knowledge base search

ğŸ† Why This Project Makes High Impact

This project proves ability in:

âœ” Applied AI & ML
âœ” Backend API engineering
âœ” Vector databases
âœ” LLM integration
âœ” Performance optimization
âœ” Explainable AI
âœ” Real product UI design
âœ” Production-level system thinking


ğŸ‘¨â€ğŸ’» Author

Satyam Shinde
Final-Year Computer Science Student
Specialization: Artificial Intelligence & Machine Learning
